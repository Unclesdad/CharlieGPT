# CharlieGPT Configuration

# Discord Bot Settings (token loaded from .env)
discord:
  command_prefix: "!"
  max_response_length: 2000

# User Settings (loaded from .env file)
# Set DISCORD_USER_ID and DISCORD_USERNAME in your .env file

# Model Settings
model:
  name: "llama-3.1-8b-instruct"  # or "qwen-2.5-8b-instruct"
  quantization: "Q4_K_M"
  context_length: 1024
  max_tokens: 200
  temperature: 0.8
  top_p: 0.9
  top_k: 40

# RAG Settings
rag:
  enabled: true
  num_contexts: 2  # Number of relevant past messages to retrieve from vector DB
  embedding_model: "all-MiniLM-L6-v2"
  similarity_threshold: 0.5
  channel_history_limit: 5  # Number of recent messages from current channel to include as immediate context

# Training Settings
training:
  base_model: "Qwen/Qwen2.5-3B-Instruct"
  output_dir: "./models/charliegpt-lora"
  epochs: 1
  batch_size: 4
  learning_rate: 0.0002
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  max_seq_length: 128

# Paths
paths:
  data_dir: "./data"
  processed_data_dir: "./processed_data"
  models_dir: "./models"
  vectordb_dir: "./vectordb"
  llama_cpp_path: "~/llama.cpp/build/bin"  # Path to llama.cpp installation
